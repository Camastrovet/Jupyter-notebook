{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bb4b819-67cf-479b-b76c-18b3e684bd4d",
   "metadata": {},
   "source": [
    "# Read Before Starting.\n",
    "\n",
    "- The cells should be executed in order.\n",
    "- To access all parts of the code, simply click on the arrowheads to the left of the titles or on the gray boxes labeled \" +... cells hidden \".\n",
    "- If any informational messages/warnings appear (without affecting the subsequent process), this will be noted in the relevant cell.\n",
    "- For cells that require modification, a comment \"# - NECESSARY MODIFICATIONS - \" will be present at the beginning of the cell (with a detailed explanation of the affected lines of code).\n",
    "- In order to avoid cluttering the cells, the details of each line of code are not explained, but a general explanatory comment is provided at the beginning of each cell.\n",
    "- Before starting, it is recommended to place all the necessary images for the processing (light frames, bias frames, dark frames, flat frames) into the same folder.\n",
    "- Before starting, save this notebook in the same folder as the one containing your raw image folder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fcf3e72-f715-4d2b-b529-734169aa38bf",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center; font-weight: bold;\">CODE</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9417d124-a859-4042-b1c9-f6bda622fb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries useful for this code\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import shutil\n",
    "\n",
    "from astropy.io import fits\n",
    "from astropy import units as u\n",
    "from astropy.nddata import CCDData\n",
    "from astropy.stats import mad_std\n",
    "from astropy.stats import sigma_clip\n",
    "from astropy.io.fits import Header\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "from ccdproc import ImageFileCollection\n",
    "import ccdproc as ccdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e2d62f-08cc-448f-bed4-3c1aef55d367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - NECESSARY MODIFICATIONS \n",
    "\n",
    "# Folder path for image collection to be processed (light frame, bias frame etc.)\n",
    "raw_path = Path('Images to process') #replace “Images to process” with the name of your folder\n",
    "raw_data = ccdp.ImageFileCollection(raw_path)\n",
    "\n",
    "# Create a folder (in the same place) for images currently being processed/processed\n",
    "calibrated_path = Path('Image processing') # if necessary, replace “Image processing” with another name as required\n",
    "calibrated_path.mkdir(exist_ok=True) \n",
    "reduced_data = ccdp.ImageFileCollection(calibrated_path)\n",
    "\n",
    "# Warnings will appear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be67e32a-9dba-4d24-8eef-663d5a342e85",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Information on the contents of the “raw” images folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f156729-7364-4543-a8b4-f1370bb37bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viewing the contents of the source folder.\n",
    "\n",
    "raw_data.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd5fc9b-5654-454c-bc15-3dcbcaa8d545",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the different frame types available in the source folder.\n",
    "\n",
    "dark_count = len(list(raw_data.hdus(imagetyp='Dark Frame')))\n",
    "bias_count = len(list(raw_data.hdus(imagetyp='Bias Frame')))\n",
    "flat_count = len(list(raw_data.hdus(imagetyp='Flat Field')))\n",
    "light_count = len(list(raw_data.hdus(imagetyp='Light Frame')))\n",
    "\n",
    "print(f\"Number of dark frames : {dark_count}\")\n",
    "print(f\"Number of bias frames : {bias_count}\")\n",
    "print(f\"Number of flat frames : {flat_count}\")\n",
    "print(f\"Number of light frames : {light_count}\")\n",
    "\n",
    "if dark_count == 0:\n",
    "    print(\"Warning: No dark frames found in the source folder!\")\n",
    "if bias_count == 0:\n",
    "    print(\"Warning: No bias frames found in the source folder!\")\n",
    "if flat_count == 0:\n",
    "    print(\"Warning: No flat frames found in the source folder!\")\n",
    "if light_count == 0:\n",
    "    print(\"Warning: No light frames found in the source folder!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15765599-f200-46f9-8a24-f23d4f4844b8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Calibration Frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4333daca-f5ca-4d34-92fb-c1c0c0f5df04",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Bias Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15671919-9534-4efb-b141-1ada9b03281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy Bias Frame to current image folder\n",
    "\n",
    "biases = raw_data.files_filtered(imagetyp='Bias Frame', include_path=True)\n",
    "\n",
    "for bias in biases:\n",
    "    shutil.copy(bias, calibrated_path)\n",
    "\n",
    "reduced_data.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a433ee9-a3e3-4f85-be4d-dcf57873caee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of “bias frames” in the images-in-progress folder\n",
    "\n",
    "calibrated_biases = reduced_data.files_filtered(imagetyp='Bias Frame', include_path=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661b205e-4773-4c06-8bd2-5a1a5b2600b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining bias frames into a Master Bias Frame (method = average) and Master Flat Frame registration.\n",
    "\n",
    "total_sum = None\n",
    "\n",
    "for bias_file in calibrated_biases:\n",
    "    data = fits.getdata(bias_file)\n",
    "    data = data.astype(np.float64) \n",
    "    if total_sum is None:\n",
    "        total_sum = data\n",
    "    else:\n",
    "        total_sum += data\n",
    "\n",
    "average_bias = total_sum / len(calibrated_biases)\n",
    "\n",
    "header = fits.getheader(calibrated_biases[0])\n",
    "header['combined'] = True\n",
    "\n",
    "combined_bias_file_name = 'Master Bias Frame.fit'\n",
    "fits.writeto(calibrated_path / combined_bias_file_name, average_bias.astype(np.float32), header=header, overwrite=True)\n",
    "\n",
    "reduced_data.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a573f8f1-dc55-419f-bef0-411641b32e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of the Bias Frame master in the images-in-progress folder.\n",
    "\n",
    "combined_bias = CCDData.read(reduced_data.files_filtered(imagetyp='Bias Frame',combined=True,include_path=True)[0],unit='adu') \n",
    "\n",
    "# Warnings will appear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e264796e-7396-439b-af93-04c8ca3b86c7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Dark Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56727870-829a-45c3-a356-25abaee681d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract the bias frame for each dark frame and store these dark frames in the images in process folder.\n",
    "\n",
    "for ccd, file_name in raw_data.ccds(imagetyp='Dark Frame',ccd_kwargs={'unit': 'adu'},return_fname=True):\n",
    "    \n",
    "    ccd = ccdp.subtract_bias(ccd, combined_bias)\n",
    "    ccd.write(calibrated_path / file_name,overwrite=True)\n",
    "\n",
    "reduced_data.refresh()\n",
    "\n",
    "# Warnings will appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b0b9be-40d3-4f20-a6ab-e4169d0bbd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining what “dark frames” are in the images-in-progress folder.\n",
    "\n",
    "calibrated_darks = reduced_data.files_filtered(imagetyp='Dark Frame', include_path=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7221d9-0fe3-4d8a-aa9d-c79c542400db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dark frame combination (method = average) and Master Dark Frame registration.\n",
    "\n",
    "total_sum = None  \n",
    "\n",
    "for dark_file in calibrated_darks:\n",
    "    data = fits.getdata(dark_file) \n",
    "    if total_sum is None:\n",
    "        total_sum = data  \n",
    "    else:\n",
    "        total_sum += data  \n",
    "\n",
    "header = fits.getheader(calibrated_darks[0])\n",
    "header['combined'] = True\n",
    "\n",
    "average_dark = total_sum / len(calibrated_darks)\n",
    "\n",
    "dark_file_name = 'Master Dark Frame.fit'\n",
    "fits.writeto(calibrated_path / dark_file_name, average_dark, header=header, overwrite=True)\n",
    "\n",
    "reduced_data.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5c48d0-f022-47b4-98ff-8b2ceaa44ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Master Dark Frame in the images-in-progress folder.\n",
    "\n",
    "combined_dark= CCDData.read(reduced_data.files_filtered(imagetyp='Dark Frame', combined=True, include_path=True)[0],unit='adu')\n",
    "\n",
    "# Information will appear.\n",
    "# Warnings will appear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c679477-d3b2-4037-a160-6d81083df0bd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Flat Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960284ea-8ea0-4b73-a457-dba04c29000c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtracting Master Bias Frame, Master Dark Frame (aligned with the exposure time of the respective flat frame), \n",
    "# Normalizing (method = average) the flat frames, then saving these flat frames in the images-in-progress folder.\n",
    "\n",
    "dark_exposure_time = combined_dark.header['exptime'] * u.second\n",
    "\n",
    "for flat_frame, file_name in raw_data.ccds(imagetyp='Flat Field', ccd_kwargs={'unit': 'adu'}, return_fname=True):\n",
    "    \n",
    "    flat_reduced = ccdp.subtract_bias(flat_frame, combined_bias)\n",
    "\n",
    "    flat_exposure_time = flat_reduced.header['exptime'] * u.second\n",
    "    scaled_dark = combined_dark.multiply(flat_exposure_time / dark_exposure_time)\n",
    "    scaled_dark.header['exptime'] = flat_exposure_time.value\n",
    "    flat_frame_reduced = ccdp.subtract_dark(flat_reduced, \n",
    "                                            scaled_dark, \n",
    "                                            exposure_time='exptime', \n",
    "                                            exposure_unit=u.second)\n",
    "\n",
    "    max_pixel_value = np.max(flat_frame_reduced.data) \n",
    "    if max_pixel_value > 0: \n",
    "        flat_frame_reduced.data /= max_pixel_value  \n",
    "\n",
    "    reduced_data_file_path = calibrated_path / Path(file_name).name  \n",
    "\n",
    "    flat_frame_reduced.write(reduced_data_file_path, overwrite=True)\n",
    "\n",
    "reduced_data.refresh()\n",
    "\n",
    "# Warnings will appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c252f83e-b5a5-4a3b-9edf-c9168f7f8906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the filters used for flat frames.\n",
    "\n",
    "flat_filters = set(h['filter'] for h in reduced_data.headers(imagetyp='Flat Field'))\n",
    "flat_filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e527477f-fc67-4445-8eb9-fdcf4ad14975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining flat frames (method = average) by filter and registering the various Master Flat Frames.\n",
    "\n",
    "for filt in flat_filters:\n",
    "    to_combine = reduced_data.files_filtered(imagetyp='Flat Field', filter=filt, include_path=True)\n",
    "\n",
    "    total_sum = sum(fits.getdata(flat_file) for flat_file in to_combine)\n",
    "    average_flat = total_sum / len(to_combine)\n",
    "\n",
    "    header = fits.getheader(to_combine[0])\n",
    "    header['combined'] = True\n",
    "\n",
    "    combined_flat_file_name = f'Master Flat Frame filter {filt}.fit'  \n",
    "\n",
    "    fits.writeto(calibrated_path / combined_flat_file_name, average_flat, header=header, overwrite=True)\n",
    "\n",
    "reduced_data.refresh()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cca0a07-2db4-4431-a8f7-26acba36ff13",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Light Frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f53b82-bc50-4295-af4c-52892e9149ec",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f62656-a305-4b0a-9dfd-16869200b34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of a dictionary (useful when multiple filters and exposure times are involved) for subsequent data processing.\n",
    "\n",
    "combined_darks = {ccd.header['exptime']: ccd for ccd in reduced_data.ccds(imagetyp='Dark Frame',ccd_kwargs={'unit': u.adu}, combined=True)}\n",
    "combined_flats = {ccd.header['filter']: ccd for ccd in reduced_data.ccds(imagetyp='Flat Field', ccd_kwargs={'unit': u.adu}, combined=True)}\n",
    "combined_bias = [ccd for ccd in reduced_data.ccds(imagetyp='Bias Frame', ccd_kwargs={'unit': u.adu}, combined=True)][0]\n",
    "\n",
    "# Information will appear.\n",
    "# Warnings will appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce86596f-f65c-4696-a203-cf701d60040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtraction of the Master Bias Frame and Master Dark Frame (aligned with the exposure time of each light frame), \n",
    "# followed by division by the Master Flat Frame. Saving the processed light frames in the 'images-in-progress' folder.\n",
    "\n",
    "all_reds = []\n",
    "light_ccds = []\n",
    "\n",
    "epsilon = 1e-6\n",
    "\n",
    "for light, file_name in raw_data.ccds(imagetyp='Light Frame', return_fname=True, ccd_kwargs=dict(unit='adu')):\n",
    "    light_ccds.append(light)\n",
    "    \n",
    "    original_header = light.header.copy()\n",
    "\n",
    "    light_reduced = ccdp.subtract_bias(light, combined_bias)\n",
    "    \n",
    "    light_exposure_time = light_reduced.header['exptime'] * u.second\n",
    "    scaled_dark = combined_dark.multiply(light_exposure_time / dark_exposure_time)\n",
    "    scaled_dark.header['exptime'] = light_exposure_time.value\n",
    "    light_frame_reduced = ccdp.subtract_dark(light_reduced,scaled_dark,exposure_time='exptime',exposure_unit=u.second)\n",
    "    \n",
    "    current_filter = light_frame_reduced.header['filter']\n",
    "    if current_filter in combined_flats:\n",
    "        good_flat = combined_flats[current_filter]\n",
    "        good_flat.data = np.where(good_flat.data == 0, epsilon, good_flat.data)\n",
    "        light_frame_reduced = ccdp.flat_correct(light_frame_reduced, good_flat)\n",
    "    else:\n",
    "        print(f\"No flat frame available for filter {current_filter}. The image  {file_name} will not be calibrated.\")\n",
    "        continue  \n",
    "    \n",
    "    light_frame_reduced.header.update(original_header)\n",
    "    \n",
    "    all_reds.append(light_frame_reduced)\n",
    "    light_frame_reduced.write(calibrated_path / file_name, overwrite=True)\n",
    "\n",
    "reduced_data.refresh()\n",
    "\n",
    "# Information will appear.\n",
    "# Warnings will appear."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b67a1c-5b89-4c2e-9099-cd7df76731e6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696f08ec-1c20-4c76-b5d0-42aea8f2e58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combination of the images with tracking of the brightest image shape and image registration.\n",
    "\n",
    "from scipy.ndimage import shift\n",
    "from skimage import measure\n",
    "\n",
    "def replace_out_of_range_pixels(image, min_threshold=0, max_threshold=65535):\n",
    "    result = image.copy()\n",
    "    out_of_range_coords = np.argwhere((result < min_threshold) | (result > max_threshold))\n",
    "    for y, x in out_of_range_coords:\n",
    "        neighbors = result[max(0, y-1):y+2, max(0, x-1):x+2]\n",
    "        valid_neighbors = neighbors[(neighbors >= min_threshold) & (neighbors <= max_threshold)]\n",
    "        if valid_neighbors.size > 0:  \n",
    "            result[y, x] = valid_neighbors.mean()\n",
    "        else:\n",
    "            result[y, x] = min_threshold if result[y, x] < min_threshold else max_threshold\n",
    "    \n",
    "    return result\n",
    "\n",
    "light_filters = set(h['filter'] for h in reduced_data.headers(imagetyp='Light Frame'))\n",
    "\n",
    "for filt in light_filters:\n",
    "    to_combine = reduced_data.files_filtered(imagetyp='Light Frame', filter=filt, include_path=True)\n",
    "    light_data = []  \n",
    "\n",
    "    reference_image = fits.getdata(to_combine[0])\n",
    "    light_data.append(reference_image)\n",
    "\n",
    "    def find_object_center(image):\n",
    "        threshold_value = np.percentile(image, 90)  # Ajuster si nécessaire\n",
    "        thresholded_image = image > threshold_value\n",
    "        labeled_image = measure.label(thresholded_image)\n",
    "        regions = measure.regionprops(labeled_image)\n",
    "        if not regions:\n",
    "            return None  \n",
    "        largest_region = max(regions, key=lambda r: r.area)\n",
    "        return largest_region.centroid\n",
    "\n",
    "    y_ref, x_ref = find_object_center(reference_image)\n",
    "    if y_ref is None or x_ref is None:\n",
    "        print(f\"No region found in the reference image for the filter {filt}.\")\n",
    "        continue\n",
    "\n",
    "    for light_file in to_combine[1:]:\n",
    "        data = fits.getdata(light_file)\n",
    "        y_curr, x_curr = find_object_center(data)\n",
    "\n",
    "        if y_curr is None or x_curr is None:\n",
    "            print(f\"No region found in image {light_file}  for filter {filt}.\")\n",
    "            continue\n",
    "\n",
    "        y_shift = int(round(y_ref - y_curr))\n",
    "        x_shift = int(round(x_ref - x_curr))\n",
    "        aligned_image = shift(data, shift=(y_shift, x_shift))\n",
    "\n",
    "        light_data.append(aligned_image)\n",
    "\n",
    "    combined_light = np.mean(light_data, axis=0)\n",
    "\n",
    "    combined_light = replace_out_of_range_pixels(combined_light)\n",
    "\n",
    "    header = fits.getheader(to_combine[0])\n",
    "    header['combined'] = True\n",
    "    combined_light_file_name = f'Combined_light_filter_{filt}_(tracking).fit'\n",
    "    fits.writeto(calibrated_path / combined_light_file_name, combined_light, header=header, overwrite=True)\n",
    "\n",
    "reduced_data.refresh()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e926767-cc39-4524-8b2f-b30df73fb1a2",
   "metadata": {},
   "source": [
    "# Image visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbfd4c0-aaa8-4622-8c31-9de9ff34cbc6",
   "metadata": {},
   "source": [
    "## Black and white"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d954da7-adc6-4fde-bca2-b2393b6156eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - NECESSARY MODIFICATIONS \n",
    "\n",
    "# Displays and saves the cropped and calibrated images.\n",
    "\n",
    "selected_filter = 'B'  # Filter example, replace with 'R', 'V', 'L', etc.\n",
    "\n",
    "combined_light_file_name = f'Combined_light_filter_{selected_filter}_(tracking).fit'\n",
    "combined_light_path = calibrated_path / combined_light_file_name\n",
    "combined_image = fits.getdata(combined_light_path)\n",
    "\n",
    "crop_margin = 20  # Adjust this number to define how many pixels to cut from the edges (optional)\n",
    "if crop_margin > 0:\n",
    "    combined_image = combined_image[crop_margin:-crop_margin, crop_margin:-crop_margin]\n",
    "\n",
    "output_file_name = f'Combined_light_grey_cropped_{selected_filter}.fit'\n",
    "fits.writeto(output_file_name, combined_image, overwrite=True)\n",
    "print(f\"Image saved in black and white in FITS format: {output_file_name}\")\n",
    "\n",
    "vmin, vmax = combined_image.min(), combined_image.max()\n",
    "plt.figure(figsize=(8, 8))\n",
    "img = plt.imshow(combined_image, cmap='gray', origin='lower', vmin=vmin, vmax=vmax)  \n",
    "plt.title(f'Combined image (min-max scale, filter {selected_filter})')\n",
    "plt.axis('off')\n",
    "\n",
    "cbar = plt.colorbar(img, orientation='vertical', fraction=0.05, pad=0.04, shrink=0.6)  \n",
    "cbar.set_label('Pixel Value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66ba0e6-c122-440b-a820-19ddc85a6adf",
   "metadata": {},
   "source": [
    "## Colors and RGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9e5286-8b38-44cf-b34b-12d18ddf6f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - NECESSARY MODIFICATIONS \n",
    "\n",
    "# Visualization of processed, cropped, and color-applied images, and saving them in the notebook folder.\n",
    "\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def apply_color_map_2(image, filter_name):\n",
    "    color_map_dict = {\n",
    "        'R': 'Reds',\n",
    "        'V': 'Greens',\n",
    "        'B': 'Blues',\n",
    "        'Halpha': 'YlOrRd',\n",
    "        'L': 'gray',\n",
    "        'I': 'hot',       \n",
    "        'U': 'plasma'    \n",
    "    }\n",
    "    color_map = color_map_dict.get(filter_name, 'viridis')  \n",
    "    \n",
    "    norm_image = (image - image.min()) / (image.max() - image.min())\n",
    "    \n",
    "    colored_image = cm.get_cmap(color_map)(norm_image)[..., :3]  \n",
    "    return colored_image\n",
    "\n",
    "selected_filter = 'R'  # Filter example, replace with 'R', 'V', 'L', etc.\n",
    "\n",
    "combined_light_file_name = f'Combined_light_filter_{selected_filter}_(tracking).fit' \n",
    "combined_light_path = calibrated_path / combined_light_file_name\n",
    "combined_image = fits.getdata(combined_light_path)\n",
    "\n",
    "colored_combined_image = apply_color_map_2(combined_image, selected_filter)\n",
    "\n",
    "crop_margin = 20  # Adjust this number to define how many pixels to cut from the edges (optional)\n",
    "if crop_margin > 0:\n",
    "    colored_combined_image = colored_combined_image[crop_margin:-crop_margin, crop_margin:-crop_margin]\n",
    "\n",
    "output_file_name = f'Combined_light_colored_{selected_filter}.png'\n",
    "plt.imsave(output_file_name, colored_combined_image)\n",
    "\n",
    "print(f\"Registered image : {output_file_name}\")\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(colored_combined_image, origin='lower')\n",
    "plt.title(f'Combined color image (filter {selected_filter})') \n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2816652d-e62b-4f90-a0e5-21deb71e74f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - NECESSARY MODIFICATIONS \n",
    "\n",
    "# Alternative colorization method (works well for the Sun, for example).\n",
    "# Visualization of processed, cropped, and color-applied images, and saving them in the notebook folder.\n",
    "\n",
    "\n",
    "def apply_color_map(image, selected_filter, lower_percentile=5, upper_percentile=95):\n",
    "    vmin = np.percentile(image, lower_percentile)\n",
    "    vmax = np.percentile(image, upper_percentile)\n",
    "    image_normalized = np.clip((image - vmin) / (vmax - vmin), 0, 1)\n",
    "\n",
    "    if selected_filter == 'U':\n",
    "        colored_image = np.zeros((*image.shape, 3))\n",
    "        colored_image[..., 2] = image_normalized * 0.9  \n",
    "        colored_image[..., 0] = image_normalized * 0.4  \n",
    "\n",
    "    elif selected_filter == 'B':\n",
    "        colored_image = np.zeros((*image.shape, 3))\n",
    "        colored_image[..., 2] = image_normalized  \n",
    "\n",
    "    elif selected_filter == 'V':\n",
    "        colored_image = np.zeros((*image.shape, 3))\n",
    "        colored_image[..., 0] = image_normalized * 0.6  \n",
    "        colored_image[..., 1] = image_normalized * 0.9  \n",
    "        colored_image[..., 2] = image_normalized * 0.7  \n",
    "\n",
    "    elif selected_filter == 'R':\n",
    "        colored_image = np.zeros((*image.shape, 3))\n",
    "        colored_image[..., 0] = image_normalized * 0.8  \n",
    "        colored_image[..., 1] = image_normalized * 0.3  \n",
    "\n",
    "    elif selected_filter == 'I':\n",
    "        colored_image = np.zeros((*image.shape, 3))\n",
    "        colored_image[..., 0] = image_normalized * 0.9  \n",
    "        colored_image[..., 1] = image_normalized * 0.1  \n",
    "\n",
    "    elif selected_filter == 'Halpha':\n",
    "        colored_image = np.zeros((*image.shape, 3))\n",
    "        colored_image[..., 0] = image_normalized * 0.8 \n",
    "        colored_image[..., 1] = image_normalized * 0.5  \n",
    "\n",
    "    elif selected_filter == 'L':\n",
    "        colored_image = np.zeros((*image.shape, 3))\n",
    "        colored_image[..., 0] = image_normalized * 0.5  \n",
    "        colored_image[..., 1] = image_normalized * 0.55  \n",
    "        colored_image[..., 2] = image_normalized * 0.5  \n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown filter: {selected_filter}\")\n",
    "\n",
    "    colored_image[image == 0] = 0 \n",
    "\n",
    "    return colored_image\n",
    "\n",
    "selected_filter = 'B'  # Filter example, replace with 'R', 'V', 'L', etc.\n",
    "\n",
    "combined_light_file_name = f'Combined_light_filter_{selected_filter}_(tracking).fit'\n",
    "combined_light_path = calibrated_path / combined_light_file_name\n",
    "combined_image = fits.getdata(combined_light_path)\n",
    "\n",
    "colored_combined_image = apply_color_map(combined_image, selected_filter)\n",
    "\n",
    "\n",
    "crop_margin = 20  # Adjust this number to define how many pixels to cut from the edges (optional)\n",
    "if crop_margin > 0:\n",
    "    colored_combined_image = colored_combined_image[crop_margin:-crop_margin, crop_margin:-crop_margin]\n",
    "\n",
    "output_file_name = f'combined_light_colored_{selected_filter}.png' \n",
    "plt.imsave(output_file_name, colored_combined_image) \n",
    "print(f\"Registered image : {output_file_name}\")\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(colored_combined_image, origin='lower')\n",
    "plt.title(f'Combined color image (filter {selected_filter})')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935ee319-1e50-4167-af46-09a1713000fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - NECESSARY MODIFICATIONS \n",
    "\n",
    "# Visualization of processed, cropped, and RGB color-applied images, and saving them in the notebook folder.\n",
    "\n",
    "from skimage import measure\n",
    "from scipy.ndimage import shift\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from astropy.io import fits\n",
    "from skimage import img_as_ubyte\n",
    "from skimage.io import imsave\n",
    "\n",
    "def read_fits(file):\n",
    "    with fits.open(file) as hdul:\n",
    "        data = hdul[0].data\n",
    "        exposure_time = hdul[0].header.get('EXPTIME', 1)\n",
    "    data[data < 0] = 0\n",
    "    data -= np.min(data)\n",
    "    data /= np.max(data)\n",
    "    return data, exposure_time\n",
    "\n",
    "def find_object_center(image):\n",
    "    threshold_value = np.percentile(image, 70) # Change percentile value if images are not aligned properly (optional)\n",
    "    thresholded_image = image > threshold_value\n",
    "    labeled_image = measure.label(thresholded_image)\n",
    "    largest_region = max(measure.regionprops(labeled_image), key=lambda r: r.area)\n",
    "    return largest_region.centroid\n",
    "\n",
    "blue_file = '/users/my_combined_image_blue_filter' # Modify the paths of combined images according to the B filters.\n",
    "green_file = '/users/my_combined_image_green_filter' # Modify the paths of combined images according to the V filters.\n",
    "red_file = '/users/my_combined_image_red_filter' # Modify the paths of combined images according to the R filters.\n",
    "red_data, red_exposure = read_fits(red_file)\n",
    "green_data, green_exposure = read_fits(green_file)\n",
    "blue_data, blue_exposure = read_fits(blue_file)\n",
    "\n",
    "min_exposure = min(red_exposure, green_exposure, blue_exposure)\n",
    "red_data *= min_exposure / red_exposure\n",
    "green_data *= min_exposure / green_exposure\n",
    "blue_data *= min_exposure / blue_exposure\n",
    "\n",
    "y_ref, x_ref = find_object_center(red_data)\n",
    "y_green, x_green = find_object_center(green_data)\n",
    "y_blue, x_blue = find_object_center(blue_data)\n",
    "\n",
    "aligned_green = shift(green_data, shift=(y_ref - y_green, x_ref - x_green))\n",
    "aligned_blue = shift(blue_data, shift=(y_ref - y_blue, x_ref - x_blue))\n",
    "aligned_red = red_data\n",
    "\n",
    "# Adjust intensities to enhance colors (optional)\n",
    "aligned_red *= 3\n",
    "aligned_green *= 2.1\n",
    "aligned_blue *= 0.2\n",
    "\n",
    "crop_margin = 20  # Adjust this number to define how many pixels to cut from the edges (optional)\n",
    "aligned_red_cropped = aligned_red[crop_margin:-crop_margin, crop_margin:-crop_margin]\n",
    "aligned_green_cropped = aligned_green[crop_margin:-crop_margin, crop_margin:-crop_margin]\n",
    "aligned_blue_cropped = aligned_blue[crop_margin:-crop_margin, crop_margin:-crop_margin]\n",
    "\n",
    "aligned_red_cropped = np.clip(aligned_red_cropped, 0, 1)\n",
    "aligned_green_cropped = np.clip(aligned_green_cropped, 0, 1)\n",
    "aligned_blue_cropped = np.clip(aligned_blue_cropped, 0, 1)\n",
    "\n",
    "rgb_image = np.dstack((aligned_red_cropped, aligned_green_cropped, aligned_blue_cropped))\n",
    "\n",
    "output_file_name_png = 'Combined_light_RGB.png'\n",
    "imsave(output_file_name_png, img_as_ubyte(rgb_image))\n",
    "print(f\"RGB image saved in PNG format: {output_file_name_png}\")\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(rgb_image, origin='lower')\n",
    "plt.xlabel('X Pixels')\n",
    "plt.ylabel('Y Pixels')\n",
    "plt.title('Image RGB')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
